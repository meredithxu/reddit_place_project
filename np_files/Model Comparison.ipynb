{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine whether to use the GradientBoostingRegressor from sklearn or a Neural Network.\n",
    "Evaluate which model gets better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import operator\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../Python_code\") # go to parent dir\n",
    "from canvas_vis import * \n",
    "from analytics_combined import *\n",
    "from generate_proj_to_remove import *\n",
    "from project_data_analysis import *\n",
    "from user_embedding import *\n",
    "from segmentation import *\n",
    "from evaluation import *\n",
    "from nonlinear_regressor import *\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "projects_to_remove = get_list_of_removed_proj(output_filename = \"../data/proj_to_remove.txt\")\n",
    "\n",
    "input_file= \"../data/sorted_tile_placements_proj.csv\"\n",
    "\n",
    "#Area of the canvas considered (0-1002,0-1002 is the full canvas)\n",
    "min_x = 450\n",
    "max_x = 550\n",
    "min_y = 450\n",
    "max_y = 550\n",
    "\n",
    "#distance threshold in update graphs\n",
    "#updates within dist_threshold positions from each other\n",
    "#that co-exist at any point in time will be connected\n",
    "dist_threshold = 1\n",
    "\n",
    "#Multithreading\n",
    "num_threads = 3\n",
    "\n",
    "#Kappa parameter for the region segmentation of the\n",
    "#update graph\n",
    "KAPPA_updates = 1.\n",
    "\n",
    "#Kappa parameter for the segmentation of the\n",
    "#region graph\n",
    "KAPPA_region = .55\n",
    "\n",
    "#User signed embedding parameters\n",
    "ndim=80\n",
    "feature_threshold=5\n",
    "total_samples=300\n",
    "n_negatives=5\n",
    "n_iterations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regions(iterations,\n",
    "                    modeltype,\n",
    "                    num_threads,\n",
    "                    input_file,\n",
    "                    projects_to_remove,\n",
    "                    dist_threshold, \n",
    "                    ndim, \n",
    "                    feature_threshold, \n",
    "                    total_samples, \n",
    "                    n_negatives, \n",
    "                    n_iterations,\n",
    "                    min_x = 0, \n",
    "                    max_x = 1002, \n",
    "                    min_y = 0, \n",
    "                    max_y = 1002,\n",
    "                    excluded_folds = [],\n",
    "                    use_scalar = True,\n",
    "                    delete_pkl_files = False\n",
    "                    ):\n",
    "    '''\n",
    "       Take all of the updates within input_file and cluster them into regions\n",
    "       PARAMS:\n",
    "            - modeltype indicated which type of model will be used to cluster the\n",
    "            updates.\n",
    "            modeltype = 'gboost' will use a GradientBoostingRegressor\n",
    "            modeltype = 'nn' will use a neural network\n",
    "            This will only affect the first iteration. All subsequent iterations will use GradientBoostingRegressor\n",
    "\n",
    "            - excluded_folds = list of values ranging between 0 to 9 inclusive\n",
    "            The folds indicted in this list will not be used to train models or generate regions\n",
    "\n",
    "            - use_scalar will use a StandardScalar to scale the data that is being used to train the first iteration model\n",
    "\n",
    "            - delete_pkl_files: boolean\n",
    "                If true, then all the saved pickle files will be deleted and regenerated\n",
    "                else the pickles will be loaded whenever possible\n",
    "\n",
    "\n",
    "\n",
    "        NOTE: This function saves several pkl files in order to avoid\n",
    "        recomputation in case of failure.\n",
    "        However, this also means that these pkl files must be manually deleted\n",
    "        if you want to regenerate the data structures.\n",
    "    '''\n",
    "\n",
    "    # Verify that the user selected a valid modeltype\n",
    "    valid_modeltypes = set(['nn', 'gboost'])\n",
    "    if not modeltype in valid_modeltypes:\n",
    "        return None\n",
    "\n",
    "    graph_filename = 'graph.pkl'\n",
    "    ups_filename = 'ups.pkl'\n",
    "    features_filename = 'features.pkl'\n",
    "    model0_filename = 'model0.pkl'\n",
    "    regions_filename = 'up_regions.pkl'\n",
    "    scalar_filenameA = 'std_scaler_A.pkl'\n",
    "    scalar_filenameb = 'std_scaler_b.pkl'\n",
    "\n",
    "    if delete_pkl_files:\n",
    "        if os.path.exists(graph_filename):\n",
    "            os.remove(graph_filename)\n",
    "        if os.path.exists(ups_filename):\n",
    "            os.remove(ups_filename)\n",
    "        if os.path.exists(features_filename):\n",
    "            os.remove(features_filename)\n",
    "        if os.path.exists(model0_filename):\n",
    "            os.remove(model0_filename)\n",
    "        if os.path.exists(regions_filename):\n",
    "            os.remove(regions_filename)\n",
    "\n",
    "        if os.path.exists(scalar_filenameA):\n",
    "            os.remove(scalar_filenameA)\n",
    "        if os.path.exists(scalar_filenameb):\n",
    "            os.remove(scalar_filenameb)\n",
    "\n",
    "    # Create a graph of the updates where each update is one node\n",
    "    # First check if this graph has already been created. If so, load the pickle\n",
    "    # Else create it and save to a pickle\n",
    "    G_ups = None\n",
    "    ups = None\n",
    "    if not (os.path.exists(graph_filename) and os.path.exists(ups_filename)):\n",
    "        t = time.time()\n",
    "        G_ups, ups = create_graph(input_file, projects_to_remove, dist_threshold, min_x, max_x, min_y, max_y, excluded_folds = excluded_folds)\n",
    "        \n",
    "        print(\"num edges = \", G_ups.n_edges)\n",
    "\n",
    "        pfile = open(graph_filename, 'wb')\n",
    "        pickle.dump(G_ups, pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        pfile = open(ups_filename, 'wb')\n",
    "        pickle.dump(ups, pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        print(\"time to create G_ups= \", time.time()-t, \" seconds\")\n",
    "    else:\n",
    "        pfile = open(graph_filename, 'rb')\n",
    "        G_ups = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        pfile = open(ups_filename, 'rb')\n",
    "        ups = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "\n",
    "    updates_proj = compute_updates_per_project(ups, False)\n",
    "    \n",
    "    fold_boundaries = None\n",
    "    if len(excluded_folds) > 0:\n",
    "        folds = create_folds(num_folds = 10, min_x = min_x, min_y = min_y, max_x = max_x, max_y = max_y)\n",
    "\n",
    "        fold_boundaries = []\n",
    "        # List of dictionaries containing min_x, max_x, min_y, max_y for each fold\n",
    "        for fold in folds:\n",
    "            fold_boundaries.append(get_fold_border(fold))\n",
    "        \n",
    "\n",
    "\n",
    "    # Create features for the updates graph\n",
    "    # First check if this features dictionary has already been created. If so, load the pickle\n",
    "    # Else create it and save to a pickle\n",
    "    features = None\n",
    "    if not os.path.exists(features_filename):\n",
    "        features = create_features(G_ups, ups, ndim, feature_threshold,\n",
    "                                   total_samples, n_negatives, n_iterations, features_filename)\n",
    "    else:\n",
    "        pfile = open(features_filename, 'rb')\n",
    "        features = pickle.load(pfile)\n",
    "        pfile.close()  \n",
    "    \n",
    "    model = None\n",
    "    if not os.path.exists(model0_filename):\n",
    "        #Creating feature matrix A and vector of labels b\n",
    "        #for learning an edge weight model\n",
    "        t = time.time()\n",
    "\n",
    "        A,b = build_feat_label_data(G_ups, ups, features, fold_boundaries = fold_boundaries, excluded_folds = excluded_folds)\n",
    "        \n",
    "        print(\"A before scaling\")\n",
    "        print(A)\n",
    "\n",
    "        if use_scalar:\n",
    "            scaler_A = StandardScaler()\n",
    "            scaler_b = StandardScaler()\n",
    "\n",
    "            scaler_A.fit(A)\n",
    "            b = np.matrix(b).T\n",
    "            scaler_b.fit(b)\n",
    "            A = scaler_A.transform(A)\n",
    "            b = (scaler_b.transform(b)).T[0]\n",
    "\n",
    "            if os.path.exists(scalar_filenameA):\n",
    "                os.remove(scalar_filenameA)\n",
    "\n",
    "            pickle.dump(scaler_A, open(scalar_filenameA, 'wb'))\n",
    "\n",
    "            if os.path.exists(scalar_filenameb):\n",
    "                os.remove(scalar_filenameb)\n",
    "            \n",
    "            pickle.dump(scaler_b, open(scalar_filenameb, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "        print(A.shape)\n",
    "        print(\"A after scaling\")\n",
    "        print(A)\n",
    "        print(b.sum() / b.shape[0])\n",
    "\n",
    "        print(\"time to build feat label data = \", time.time()-t, \" seconds\")\n",
    "\n",
    "        t = time.time()\n",
    "        if modeltype == 'gboost':\n",
    "            model = GradientBoostingRegressor(random_state=1, n_estimators=25).fit(A, b)\n",
    "        else:\n",
    "            # model = createNonlinearRegressionNeuralNet(A, b, train_proportion = 0.8, first_nodes=64, second_nodes=32, dropout = 0)\n",
    "            model = MLPRegressor(max_iter=100, verbose=True, early_stopping=True).fit(A, b)\n",
    "        pfile = open(model0_filename, 'wb')\n",
    "        pickle.dump(model, pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        print(\"time to fit model= \", time.time()-t, \" seconds\")\n",
    "    else:\n",
    "        pfile = open(model0_filename, 'rb')\n",
    "        model = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "                \n",
    "    regions = None\n",
    "    sizes = None\n",
    "    int_weights = None\n",
    "    if not os.path.exists(regions_filename):\n",
    "        #Computing edge weights using multithreading\n",
    "        #and sorting the edges in increasing order.\n",
    "        #Not every edge in the the unique_edges file\n",
    "        #will be included in the new file.\n",
    "\n",
    "        t = time.time()\n",
    "        if use_scalar:\n",
    "            compute_edge_weights_multithread(G_ups, ups, model, features, features_filename, num_threads, scalar_filenameA, scalar_filenameb)\n",
    "        else:\n",
    "            compute_edge_weights_multithread(G_ups, ups, model, features, features_filename, num_threads)\n",
    "        G_ups.sort_edges()\n",
    "\n",
    "        print(\"time to compute edge weights= \", time.time()-t, \" seconds\")\n",
    "\n",
    "        #Performing region segmentation on the update graph\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # comp_assign, int_weights = region_segmentation(G_ups, ups, 0.8)\n",
    "        # regions, sizes, int_weights = extract_regions(comp_assign, int_weights)\n",
    "        regions, sizes, int_weights = superv_reg_segm_ups(G_ups, ups, 0., 2., updates_proj, recall)\n",
    "\n",
    "        pfile = open(regions_filename, 'wb')\n",
    "        pickle.dump([regions, sizes, int_weights], pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        print(\"num regions = \", len(regions), \" max size region = \", np.max(sizes))\n",
    "\n",
    "        print(\"time to create regions= \", time.time()-t, \" seconds\")\n",
    "    else:\n",
    "        pfile = open(regions_filename, 'rb')\n",
    "        ups_region_info = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        regions = ups_region_info[0]\n",
    "        sizes = ups_region_info[1]\n",
    "        int_weights = ups_region_info[2]\n",
    "     \n",
    "    durations = compute_update_durations(ups)\n",
    "    for i in range(1, iterations):\n",
    "        reg_graph_filename = \"reg_graph\" + str(i) + \".pkl\"\n",
    "        reg_features_filename = \"features_regions\" + str(i) + \".pkl\"\n",
    "        reg_model_filename = 'model_regions' + str(i) + '.pkl'\n",
    "        super_regions_filename = 'super_regions' + str(i) + '.pkl'\n",
    "\n",
    "        if delete_pkl_files:\n",
    "            if os.path.exists(reg_graph_filename):\n",
    "                os.remove(reg_graph_filename)\n",
    "            if os.path.exists(reg_features_filename):\n",
    "                os.remove(reg_features_filename)\n",
    "            if os.path.exists(reg_model_filename):\n",
    "                os.remove(reg_model_filename)\n",
    "            if os.path.exists(super_regions_filename):\n",
    "                os.remove(super_regions_filename)\n",
    "\n",
    "        G_reg = None\n",
    "\n",
    "        if not os.path.exists(reg_graph_filename):\n",
    "            t = time.time()\n",
    "\n",
    "            G_reg = build_region_graph(G_ups, regions, ups, .5, projects_to_remove)\n",
    "\n",
    "            pfile = open(reg_graph_filename, 'wb')\n",
    "            pickle.dump(G_reg, pfile)\n",
    "            pfile.close()\n",
    "\n",
    "            print(\"time to create reg_graph\",i,\"= \", time.time()-t, \" seconds\")\n",
    "        else:\n",
    "            pfile = open(reg_graph_filename, 'rb')\n",
    "            G_reg = pickle.load(pfile)\n",
    "            pfile.close()\n",
    "\n",
    "        region_features = None\n",
    "        if not os.path.exists(reg_features_filename):\n",
    "            region_features = create_superfeatures(\n",
    "                regions, int_weights, ups, features, durations, reg_features_filename)\n",
    "            \n",
    "        else:\n",
    "            #Reading existing feature data\n",
    "            pfile = open(reg_features_filename, 'rb')\n",
    "            region_features = pickle.load(pfile)\n",
    "            pfile.close()\n",
    "        \n",
    "        model = None\n",
    "\n",
    "        if not os.path.exists(reg_model_filename):\n",
    "            t = time.time()\n",
    "\n",
    "            A, b = build_feat_label_regions(G_reg, ups, region_features)\n",
    "\n",
    "            print(A.shape)\n",
    "            print(b.sum() / b.shape[0])\n",
    "\n",
    "            print(\"time to build feat label data\",i,\"= \", time.time()-t, \" seconds\")\n",
    "\n",
    "            #Learning region edge weight model using gradient boosting regression\n",
    "            #and saving model.\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "            model = GradientBoostingRegressor(random_state=1, n_estimators=25).fit(A, b)\n",
    "\n",
    "            pfile = open(reg_model_filename, 'wb')\n",
    "            pickle.dump(model, pfile)\n",
    "            pfile.close()\n",
    "\n",
    "            print(\"time to fit model\", i, \"= \", time.time()-t, \" seconds\")\n",
    "        else:\n",
    "            #Reading an existing region edge weight model\n",
    "\n",
    "            pfile = open(reg_model_filename, 'rb')\n",
    "            model = pickle.load(pfile)\n",
    "            pfile.close()\n",
    "\n",
    "        \n",
    "        #Computing region edge weights using multithreading\n",
    "        #and sorting the edges in increasing order.\n",
    "        super_region_sizes = None\n",
    "        \n",
    "        if not os.path.exists(super_regions_filename):\n",
    "            t = time.time()\n",
    "\n",
    "            compute_edge_weights_multithread(G_reg, ups, model, region_features, reg_features_filename, num_threads)\n",
    "            G_reg.sort_edges()\n",
    "\n",
    "            print(\"time compute edge weights\",i,\"= \", time.time()-t, \" seconds\")\n",
    "\n",
    "            #Performing segmentation on the region graph\n",
    "            t = time.time()\n",
    "\n",
    "            # comp_assign_reg, int_weights_reg = region_segmentation(G_reg, regions, 0.55)\n",
    "            # reg_regions, reg_sizes, int_weights = extract_regions(comp_assign_reg, int_weights_reg)\n",
    "            # regions, super_region_sizes, super_region_assign = extract_super_region_info(reg_regions, regions)\n",
    "            regions, super_region_sizes, int_weights = superv_reg_segm_reg(G_reg, ups, regions, 0., 2., updates_proj, recall)\n",
    "\n",
    "            pfile = open(super_regions_filename, 'wb')\n",
    "            pickle.dump([regions, super_region_sizes, int_weights], pfile)\n",
    "            pfile.close()\n",
    "\n",
    "            print(\"time to create super regions\",i,\"= \", time.time()-t, \" seconds\")\n",
    "        else:\n",
    "            pfile = open(super_regions_filename, 'rb')\n",
    "            super_region_info = pickle.load(pfile)\n",
    "            pfile.close()\n",
    "\n",
    "            regions = super_region_info[0]\n",
    "            super_region_sizes = super_region_info[1]\n",
    "            int_weights = super_region_info[2]\n",
    "        \n",
    "        print(\"num regions = \", len(regions), \" max size region = \", np.max(super_region_sizes))\n",
    "\n",
    "\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num edges =  1747975\n",
      "time to create G_ups=  130.19095659255981  seconds\n",
      "python ../../signet/signet.py -l signet_id.txt -i signet.txt -o signet -d 80 -t 5 -s 300\n",
      "balanced:\n",
      "+++  0.8509881685214751  rand =  0.34161980874689163\n",
      "+--  0.08046849688306315  rand =  0.1899310287891106\n",
      "unbalanced:\n",
      "++-  0.06119599033495848  rand =  0.44119452080698934\n",
      "---  0.007347344260503316  rand =  0.0272546416570083\n",
      "avg pos =  0.8233456984688834 , n =  662873\n",
      "avg neg =  1.7596048169773761 , n =  285362\n",
      "time to create G_ups features=  900.0313775539398  seconds\n",
      "A before scaling\n",
      "[[0.         1.         0.03194444 0.         0.86627982 0.48888889]\n",
      " [0.         1.         0.0725     0.         0.70907629 0.14285714]\n",
      " [0.         1.41421356 0.09       0.         0.91005362 0.47191011]\n",
      " ...\n",
      " [0.         1.41421356 0.70583333 0.         1.2265822  0.42105263]\n",
      " [0.         1.         0.08277778 0.         0.63661309 0.48333333]\n",
      " [0.         1.41421356 0.71583333 0.         1.0707664  0.6       ]]\n",
      "(626380, 6)\n",
      "A after scaling\n",
      "[[-0.41177911  0.26042409 -0.29322591 -0.20608499  0.0424283   0.52307816]\n",
      " [-0.41177911  0.26042409 -0.27977303 -0.20608499 -0.43836775 -1.04336138]\n",
      " [-0.41177911  0.98213867 -0.27396802 -0.20608499  0.17630744  0.4462175 ]\n",
      " ...\n",
      " [-0.41177911  0.98213867 -0.06968696 -0.20608499  1.14438805  0.21599247]\n",
      " [-0.41177911  0.26042409 -0.27636374 -0.20608499 -0.65999137  0.4979289 ]\n",
      " [-0.41177911  0.98213867 -0.06636982 -0.20608499  0.66783626  1.02606333]]\n",
      "-3.666263485386896e-17\n",
      "time to build feat label data =  545.2774171829224  seconds\n",
      "Iteration 1, loss = 0.38384860\n",
      "Validation score: 0.248690\n",
      "Iteration 2, loss = 0.37762607\n",
      "Validation score: 0.258729\n",
      "Iteration 3, loss = 0.37560664\n",
      "Validation score: 0.260579\n",
      "Iteration 4, loss = 0.37428418\n",
      "Validation score: 0.264873\n",
      "Iteration 5, loss = 0.37380079\n",
      "Validation score: 0.268758\n",
      "Iteration 6, loss = 0.37292385\n",
      "Validation score: 0.266600\n",
      "Iteration 7, loss = 0.37232736\n",
      "Validation score: 0.271267\n",
      "Iteration 8, loss = 0.37195751\n",
      "Validation score: 0.265693\n",
      "Iteration 9, loss = 0.37127712\n",
      "Validation score: 0.269604\n",
      "Iteration 10, loss = 0.37086311\n",
      "Validation score: 0.273857\n",
      "Iteration 11, loss = 0.37044061\n",
      "Validation score: 0.273451\n",
      "Iteration 12, loss = 0.37013611\n",
      "Validation score: 0.270817\n",
      "Iteration 13, loss = 0.36959217\n",
      "Validation score: 0.276927\n",
      "Iteration 14, loss = 0.36931369\n",
      "Validation score: 0.274352\n",
      "Iteration 15, loss = 0.36907458\n",
      "Validation score: 0.272952\n",
      "Iteration 16, loss = 0.36871518\n",
      "Validation score: 0.276786\n",
      "Iteration 17, loss = 0.36872408\n",
      "Validation score: 0.273233\n",
      "Iteration 18, loss = 0.36828060\n",
      "Validation score: 0.279300\n",
      "Iteration 19, loss = 0.36785605\n",
      "Validation score: 0.276465\n",
      "Iteration 20, loss = 0.36803484\n",
      "Validation score: 0.275834\n",
      "Iteration 21, loss = 0.36802778\n",
      "Validation score: 0.278569\n",
      "Iteration 22, loss = 0.36750059\n",
      "Validation score: 0.277528\n",
      "Iteration 23, loss = 0.36716154\n",
      "Validation score: 0.274514\n",
      "Iteration 24, loss = 0.36701355\n",
      "Validation score: 0.278949\n",
      "Iteration 25, loss = 0.36668025\n",
      "Validation score: 0.279610\n",
      "Iteration 26, loss = 0.36677143\n",
      "Validation score: 0.280525\n",
      "Iteration 27, loss = 0.36677268\n",
      "Validation score: 0.280698\n",
      "Iteration 28, loss = 0.36638380\n",
      "Validation score: 0.270541\n",
      "Iteration 29, loss = 0.36639723\n",
      "Validation score: 0.282487\n",
      "Iteration 30, loss = 0.36628135\n",
      "Validation score: 0.279285\n",
      "Iteration 31, loss = 0.36609414\n",
      "Validation score: 0.281883\n",
      "Iteration 32, loss = 0.36594223\n",
      "Validation score: 0.280184\n",
      "Iteration 33, loss = 0.36596826\n",
      "Validation score: 0.281719\n",
      "Iteration 34, loss = 0.36583122\n",
      "Validation score: 0.280104\n",
      "Iteration 35, loss = 0.36571425\n",
      "Validation score: 0.281249\n",
      "Iteration 36, loss = 0.36536000\n",
      "Validation score: 0.270680\n",
      "Iteration 37, loss = 0.36563581\n",
      "Validation score: 0.275911\n",
      "Iteration 38, loss = 0.36528274\n",
      "Validation score: 0.278263\n",
      "Iteration 39, loss = 0.36521942\n",
      "Validation score: 0.281911\n",
      "Iteration 40, loss = 0.36504612\n",
      "Validation score: 0.279848\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "time to fit model=  311.2568738460541  seconds\n",
      "Feature shape: (484632, 6)\n",
      "result shape: (484632,)\n",
      "Feature shape: (484631, 6)\n",
      "Feature shape: (484631, 6)\n",
      "result shape: (484631,)\n",
      "result shape: (484631,)\n",
      "time to compute edge weights=  461.87451100349426  seconds\n",
      "kappa =  0.3819660112501051  score =  0.11764705882352941\n",
      "kappa =  0.6180339887498949  score =  0.058823529411764705\n",
      "kappa =  0.2360679774997897  score =  0.0\n",
      "kappa =  0.47213595499957944  score =  0.11764705882352941\n",
      "kappa =  0.5278640450004206  score =  0.11764705882352941\n",
      "kappa =  0.5623058987490537  score =  0.11764705882352941\n",
      "kappa =  0.5835921350012618  score =  0.11764705882352941\n",
      "kappa =  0.5967477524976867  score =  0.11764705882352941\n",
      "kappa =  0.60487837125347  score =  0.11764705882352941\n",
      "kappa =  0.6099033699941115  score =  0.11764705882352941\n",
      "kappa =  0.6130089900092534  score =  0.11764705882352941\n",
      "kappa =  0.6149283687347531  score =  0.11764705882352941\n",
      "kappa =  0.6161146100243952  score =  0.058823529411764705\n",
      "kappa =  0.6141952312988955  score =  0.11764705882352941\n",
      "kappa =  0.6153814725885377  score =  0.11764705882352941\n",
      "kappa =  0.6156615061706106  score =  0.11764705882352941\n",
      "kappa =  0.6158345764423225  score =  0.11764705882352941\n",
      "kappa =  0.6159415397526832  score =  0.058823529411764705\n",
      "kappa =  0.6157684694809709  score =  0.11764705882352941\n",
      "kappa =  0.6158754327913312  score =  0.058823529411764705\n",
      "kappa =  0.6158093258299793  score =  0.11764705882352941\n",
      "kappa =  0.6158501821789877  score =  0.058823529411764705\n",
      "kappa =  0.6158249315666443  score =  0.11764705882352941\n",
      "kappa =  0.6158405373033093  score =  0.11764705882352941\n",
      "kappa =  0.615844221318001  score =  0.11764705882352941\n",
      "score =  0.11764705882352941\n",
      "num regions =  61957  max size region =  332132\n",
      "time to create regions=  286.8873608112335  seconds\n"
     ]
    }
   ],
   "source": [
    "regions = create_regions(1,\n",
    "                    \"nn\",\n",
    "                    num_threads,\n",
    "                    input_file,\n",
    "                    projects_to_remove,\n",
    "                    dist_threshold, \n",
    "                    ndim, \n",
    "                    feature_threshold, \n",
    "                    total_samples, \n",
    "                    n_negatives, \n",
    "                    n_iterations,\n",
    "                    min_x = min_x, \n",
    "                    max_x = max_x, \n",
    "                    min_y = min_y, \n",
    "                    max_y = max_y,\n",
    "                    use_scalar = True,\n",
    "                    delete_pkl_files = True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY THE CODE ABOVE THIS IS USED FOR RUNNING MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the size of the canvas that is being looked at\n",
    "min_x = 0\n",
    "max_x = 10\n",
    "min_y = 0\n",
    "max_y = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim=80\n",
    "threshold=5\n",
    "total_samples=300\n",
    "n_negatives=5\n",
    "n_iterations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_to_remove = get_list_of_removed_proj(output_filename = \"../data/proj_to_remove.txt\")\n",
    "\n",
    "input_file= \"../data/sorted_tile_placements_proj.csv\"\n",
    "js_filename = \"../data/atlas_complete.json\"\n",
    "\n",
    "names, descriptions = read_picture_names_and_descriptions(js_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"10x10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num edges =  189495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TODO: How far two vertices should be to be connected (1-4)?\n",
    "G, ups = create_graph(input_file, projects_to_remove, 4, min_x, max_x, min_y, max_y, file_prefix=\"comparison\")\n",
    "\n",
    "print(\"num edges = \", G.n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'graph.pkl', 'wb')\n",
    "pickle.dump(G, pfile)\n",
    "pfile.close()\n",
    "\n",
    "pfile = open(file_prefix + 'ups.pkl', 'wb')\n",
    "pickle.dump(ups, pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'graph.pkl', 'rb')\n",
    "G = pickle.load(pfile)\n",
    "pfile.close()\n",
    "\n",
    "pfile = open(file_prefix + 'ups.pkl', 'rb')\n",
    "ups = pickle.load(pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining 7 edge features and computing the information they require\n",
    "#Adding a new feature without changing the rest of the code should\n",
    "#be easy.\n",
    "#TODO: Are there other features that would improve the segmentation?\n",
    "#TODO: How many dimensions we need?\n",
    "\n",
    "#Functions that compute edge features for the\n",
    "#update graph. The functions receive indexes\n",
    "#of a pair of updates and some necessary data\n",
    "#and return a real value.\n",
    "\n",
    "def different_color(i, j, ups, data=None):\n",
    "    '''\n",
    "        Simply checks if updates have different color.\n",
    "    '''\n",
    "    if ups[i][4] == ups[j][4]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "def distance_space(i, j, ups, data=None):\n",
    "    '''\n",
    "        Eclidean distance between updates.\n",
    "    '''\n",
    "    xi = ups[i][2]\n",
    "    yi = ups[i][3]\n",
    "    xj = ups[j][2]\n",
    "    yj = ups[j][3]\n",
    "    \n",
    "    return np.sqrt(pow(xi-xj,2)+pow(yi-yj,2))\n",
    "\n",
    "def distance_time(i, j, ups, data=None):\n",
    "    '''\n",
    "        Time distance between updates\n",
    "        in hours.\n",
    "    '''\n",
    "    time_i = ups[i][0]\n",
    "    time_j = ups[j][0]\n",
    "    \n",
    "    return np.abs(time_i-time_j) / 3600000 #hours\n",
    "\n",
    "def distance_duration(i, j, ups, durations):\n",
    "    '''\n",
    "        Distance between duration of updates.\n",
    "        See function dist_duration for details.\n",
    "    '''\n",
    "    return dist_duration(durations[i], durations[j])\n",
    "\n",
    "def distance_color(i, j, ups, conflicts):\n",
    "    '''\n",
    "        Computes the distance between two colors\n",
    "        based on how often one has replaced the\n",
    "        other in that particular position (x,y).\n",
    "    '''\n",
    "    color_i = ups[i][4]\n",
    "    color_j = ups[j][4]\n",
    "    \n",
    "    if color_i == color_j:\n",
    "        return 0\n",
    "    else:\n",
    "        max_up = len(ups)\n",
    "        dist = 0\n",
    "        \n",
    "        conf_i = []\n",
    "        if conflicts[i][0] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][0]][4])\n",
    "            \n",
    "        if conflicts[i][1] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][1]][4])\n",
    "        \n",
    "        conf_j = []\n",
    "        if conflicts[j][0] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][0]][4])\n",
    "            \n",
    "        if conflicts[j][1] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][1]][4])\n",
    "        \n",
    "        if color_i in conf_j:\n",
    "            dist = dist + 1\n",
    "            \n",
    "        if color_j in conf_i:\n",
    "            dist = dist + 1\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "def distance_user_embedding(i, j, ups, data):\n",
    "    '''\n",
    "        Euclidean distance between user embeddings.\n",
    "    '''\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return np.linalg.norm(data['emb'][user_i_id]-data['emb'][user_j_id])\n",
    "\n",
    "def distance_user_colors(i, j, ups, data):\n",
    "    '''\n",
    "        Distance between user color histograms.\n",
    "        One minus sum of minimum values for each\n",
    "        color.\n",
    "    '''\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return 1.-data['emb'][user_i_id].minimum(data['emb'][user_j_id]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../../signet/signet.py -l signet_id.txt -i signet.txt -o signet -d 80 -t 5 -s 300\n",
      "balanced:\n",
      "+++  0.8138598595168751  rand =  0.33582912434093615\n",
      "+--  0.14665067671749185  rand =  0.19387030070573913\n",
      "unbalanced:\n",
      "++-  0.028696248072640054  rand =  0.44195235033334423\n",
      "---  0.010793215692992976  rand =  0.028348224619980388\n",
      "avg pos =  0.4156856997368502 , n =  18677\n",
      "avg neg =  1.8777767123211635 , n =  8193\n",
      "time =  133.08341026306152  seconds\n"
     ]
    }
   ],
   "source": [
    "#Prepares data for feature computation and saves it.\n",
    "#Takes a long time.\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "conflicts = compute_update_conflicts(ups)\n",
    "durations = compute_update_durations(ups)\n",
    "user_color, user_index_color = compute_user_color(ups)\n",
    "\n",
    "user_index, emb = embed_users(G, ups, ndim, threshold, total_samples, n_negatives, n_iterations, True)\n",
    "\n",
    "features = [{'name': \"different_color\", 'func': different_color, 'data': None}, \n",
    "    {'name': \"distance_space\",  'func': distance_space, 'data': None}, \n",
    "    {'name': \"distance_time\", 'func': distance_time, 'data': None}, \n",
    "    {'name': \"distance_color\", 'func': distance_color, 'data': conflicts},\n",
    "    {'name': \"distance_user_embedding\", 'func': distance_user_embedding, 'data': {'index': user_index, 'emb': emb}},\n",
    "    {'name': \"distance_user_colors\", 'func': distance_user_colors, 'data': {'index': user_index_color, 'emb': user_color}}]\n",
    "\n",
    "print(\"time = \", time.time()-t, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'features.pkl', 'wb')\n",
    "pickle.dump(features, pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'features.pkl', 'rb')\n",
    "features = pickle.load(pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = store_locations(\"../data/atlas_complete.json\")\n",
    "folds = create_folds(min_x, min_y, max_x, max_y)\n",
    "\n",
    "# List of dictionaries containing min_x, max_x, min_y, max_y for each fold\n",
    "fold_boundaries = []\n",
    "for fold in folds:\n",
    "    fold_boundaries.append(get_fold_border(fold))\n",
    "\n",
    "# All edges that belong to the validation fold need to be excluded\n",
    "A_train, b_train = build_feat_label_data(G, ups, features, fold_boundaries=fold_boundaries, excluded_folds=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40292,)\n",
      "(40292, 6)\n",
      "(40292,)\n",
      "(40292, 6)\n"
     ]
    }
   ],
   "source": [
    "# Trying standard scaler \n",
    "scaler_A = StandardScaler()\n",
    "scaler_b = StandardScaler()\n",
    "\n",
    "scaler_A.fit(A_train)\n",
    "scaled_A = scaler_A.transform(A_train)\n",
    "\n",
    "transformed_b = np.matrix(b_train).T\n",
    "scaler_b.fit(transformed_b)\n",
    "scaled_b = scaler_b.transform(transformed_b)\n",
    "scaled_b = scaled_b.T[0]\n",
    "\n",
    "print(scaled_b.shape)\n",
    "print(scaled_A.shape)\n",
    "print(b_train.shape)\n",
    "print(A_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying minmax scaler\n",
    "# scaler_A = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# scaler_A.fit(A_train)\n",
    "# scaled_A = scaler_A.transform(A_train)\n",
    "\n",
    "# transformed_b = np.matrix(b_train).T\n",
    "# scaler_b.fit(transformed_b)\n",
    "# scaled_b = scaler_b.transform(transformed_b)\n",
    "# scaled_b = scaled_b.T[0]\n",
    "\n",
    "# print(scaled_b.shape)\n",
    "# print(scaled_A.shape)\n",
    "# print(b_train.shape)\n",
    "# print(A_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerA_filename = \"scaler_A.pkl\"\n",
    "scalerb_filename = \"scaler_b.pkl\"\n",
    "pickle.dump(scaler_A, open(scalerA_filename, 'wb'))\n",
    "pickle.dump(scaler_b, open(scalerb_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gboost = GradientBoostingRegressor(random_state=1, n_estimators=25).fit(scaled_A, scaled_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32233 samples, validate on 8059 samples\n",
      "Epoch 1/256\n",
      " - 28s - loss: 6.7114e-04 - val_loss: 9.0966e-07\n",
      "Epoch 2/256\n",
      " - 29s - loss: 6.1046e-08 - val_loss: 3.7310e-12\n",
      "Epoch 3/256\n",
      " - 28s - loss: 9.6645e-10 - val_loss: 3.5261e-12\n",
      "Epoch 4/256\n",
      " - 28s - loss: 7.7539e-10 - val_loss: 3.4856e-12\n",
      "Epoch 5/256\n",
      " - 28s - loss: 8.0397e-10 - val_loss: 8.7282e-10\n",
      "Epoch 6/256\n",
      " - 28s - loss: 7.9980e-10 - val_loss: 1.5783e-09\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_nn = createNonlinearRegressionNeuralNet(scaled_A, scaled_b, train_proportion = 0.8, first_nodes=64, second_nodes=32, dropout = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gboost.train_score_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.9663657e-05]\n",
      " [3.9663657e-05]\n",
      " [3.9663657e-05]\n",
      " ...\n",
      " [3.9663657e-05]\n",
      " [3.9663657e-05]\n",
      " [3.9663657e-05]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model_nn.predict(scaled_A)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in prediction:\n",
    "    if i < 0:\n",
    "        count+=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in scaled_b:\n",
    "    if i < 0:\n",
    "        count+=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = create_ground_truth(input_file, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y, projects_to_remove=projects_to_remove)\n",
    "\n",
    "fold_boundaries = []\n",
    "for fold in folds:\n",
    "    fold_boundaries.append(get_fold_border(fold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (36208, 6)\n",
      "result shape: (36208,)\n",
      "Feature shape: (36207, 6)\n",
      "result shape: (36207,)\n",
      "Feature shape: (36207, 6)\n",
      "result shape: (36207,)\n",
      "Feature shape: (36207, 6)\n",
      "result shape: (36207,)\n",
      "Feature shape: (36208, 6)\n",
      "result shape: (36208,)\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "kappa = 0.25\n",
    "\n",
    "compute_edge_weights_multithread(G, ups, model_gboost, features, file_prefix + 'features.pkl', 5, scalerX=scalerA_filename, scalerY=scalerb_filename)\n",
    "G.sort_edges()\n",
    "\n",
    "\n",
    "\n",
    "comp_assign, int_weights = region_segmentation(G, ups, kappa)\n",
    "regions, sizes, int_weights = extract_regions(comp_assign, int_weights)\n",
    "num_correct_counter, num_assignments_made, precision, recall = evaluate(locations, regions, ups, ground_truth, threshold=0.3, min_x=fold_boundaries[0][\"min_x\"], max_x=fold_boundaries[0][\"max_x\"], min_y=fold_boundaries[0][\"min_y\"], max_y=fold_boundaries[0][\"max_y\"])\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (181037, 6)\n",
      "result shape: (181037, 1)\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "compute_edge_weights(G, ups, model_nn, features, scalerX=scalerA_filename, scalerY=scalerb_filename)\n",
    "G.sort_edges()\n",
    "\n",
    "\n",
    "\n",
    "comp_assign, int_weights = region_segmentation(G, ups, kappa)\n",
    "regions, sizes, int_weights = extract_regions(comp_assign, int_weights)\n",
    "num_correct_counter, num_assignments_made, precision, recall = evaluate(locations, regions, ups, ground_truth, threshold=0.3, min_x=fold_boundaries[0][\"min_x\"], max_x=fold_boundaries[0][\"max_x\"], min_y=fold_boundaries[0][\"min_y\"], max_y=fold_boundaries[0][\"max_y\"])\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
