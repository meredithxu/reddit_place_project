{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine whether to use the GradientBoostingRegressor from sklearn or a Neural Network.\n",
    "Evaluate which model gets better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import operator\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../Python_code\") # go to parent dir\n",
    "from canvas_vis import * \n",
    "from analytics_combined import *\n",
    "from generate_proj_to_remove import *\n",
    "from project_data_analysis import *\n",
    "from user_embedding import *\n",
    "from segmentation import *\n",
    "from evaluation import *\n",
    "from nonlinear_regressor import *\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the size of the canvas that is being looked at\n",
    "min_x = 450\n",
    "max_x = 550\n",
    "min_y = 450\n",
    "max_y = 550\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_to_remove = get_list_of_removed_proj(output_filename = \"../data/proj_to_remove.txt\")\n",
    "\n",
    "input_file= \"../data/sorted_tile_placements_proj.csv\"\n",
    "js_filename = \"../data/atlas_complete.json\"\n",
    "\n",
    "names, descriptions = read_picture_names_and_descriptions(js_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"10x10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num edges =  12274043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TODO: How far two vertices should be to be connected (1-4)?\n",
    "G, ups = create_graph(input_file, projects_to_remove, 4, min_x, max_x, min_y, max_y, file_prefix=\"comparison\")\n",
    "\n",
    "print(\"num edges = \", G.n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'graph.pkl', 'wb')\n",
    "pickle.dump(G, pfile)\n",
    "pfile.close()\n",
    "\n",
    "pfile = open(file_prefix + 'ups.pkl', 'wb')\n",
    "pickle.dump(ups, pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'graph.pkl', 'rb')\n",
    "G = pickle.load(pfile)\n",
    "pfile.close()\n",
    "\n",
    "pfile = open(file_prefix + 'ups.pkl', 'rb')\n",
    "ups = pickle.load(pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining 7 edge features and computing the information they require\n",
    "#Adding a new feature without changing the rest of the code should\n",
    "#be easy.\n",
    "#TODO: Are there other features that would improve the segmentation?\n",
    "#TODO: How many dimensions we need?\n",
    "\n",
    "def different_color(i, j, ups, data=None):\n",
    "    if ups[i][4] == ups[j][4]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "def distance_space(i, j, ups, data=None):\n",
    "    xi = ups[i][2]\n",
    "    yi = ups[i][3]\n",
    "    xj = ups[j][2]\n",
    "    yj = ups[j][3]\n",
    "    \n",
    "    return np.sqrt(pow(xi-xj,2)+pow(yi-yj,2))\n",
    "\n",
    "def distance_time(i, j, ups, data=None):\n",
    "    time_i = ups[i][0]\n",
    "    time_j = ups[j][0]\n",
    "    \n",
    "    return np.sqrt(pow(time_i-time_j,2))\n",
    "\n",
    "def distance_duration(i, j, ups, durations):\n",
    "    return dist_duration(durations[i], durations[j])\n",
    "\n",
    "def distance_color(i, j, ups, conflicts):\n",
    "    color_i = ups[i][4]\n",
    "    color_j = ups[j][4]\n",
    "    \n",
    "    if color_i == color_j:\n",
    "        return 0\n",
    "    else:\n",
    "        max_up = len(ups)\n",
    "        dist = 0\n",
    "        \n",
    "        conf_i = []\n",
    "        if conflicts[i][0] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][0]][4])\n",
    "            \n",
    "        if conflicts[i][1] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][1]][4])\n",
    "        \n",
    "        conf_j = []\n",
    "        if conflicts[j][0] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][0]][4])\n",
    "            \n",
    "        if conflicts[j][1] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][1]][4])\n",
    "        \n",
    "        if color_i in conf_j:\n",
    "            dist = dist + 1\n",
    "            \n",
    "        if color_j in conf_i:\n",
    "            dist = dist + 1\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "def distance_user_embedding(i, j, ups, data):\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return np.linalg.norm(data['emb'][user_i_id]-data['emb'][user_j_id])\n",
    "\n",
    "def distance_user_colors(i, j, ups, data):\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return (1.-data['emb'][user_i_id].todense() * data['emb'][user_j_id].todense().T)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../../signet/signet.py -l signet_id.txt -i signet.txt -o signet -d 40 -t 10 -s 200\n",
      "reddit_place_project\n",
      "signet\n",
      "\n",
      "avg pos =  0.6544713490067806 , n =  3363278\n",
      "avg neg =  1.5683112641071626 , n =  292678\n"
     ]
    }
   ],
   "source": [
    "conflicts = compute_update_conflicts(ups)\n",
    "durations = compute_update_durations(ups)\n",
    "user_color, user_index_color = compute_user_color(ups)\n",
    "\n",
    "#TODO: We are currently using 40 dimensions, we might need more\n",
    "# We also need to understand whether these other parameters matter.\n",
    "ndim=40\n",
    "threshold=10\n",
    "total_samples=200\n",
    "n_negatives=5\n",
    "n_iterations=10\n",
    "user_index, emb = embed_users(G, ups, ndim, threshold, total_samples, n_negatives, n_iterations)\n",
    "\n",
    "features = [{'name': \"different_color\", 'func': different_color, 'data': None}, \n",
    "    {'name': \"distance_space\",  'func': distance_space, 'data': None}, \n",
    "    {'name': \"distance_time\", 'func': distance_time, 'data': None}, \n",
    "    {'name': \"distance_duration\", 'func': distance_duration, 'data': durations}, \n",
    "    {'name': \"distance_color\", 'func': distance_color, 'data': conflicts},\n",
    "    {'name': \"distance_user_embedding\", 'func': distance_user_embedding, 'data': {'index': user_index, 'emb': emb}},\n",
    "    {'name': \"distance_user_colors\", 'func': distance_user_colors, 'data': {'index': user_index_color, 'emb': user_color}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'features.pkl', 'wb')\n",
    "pickle.dump(features, pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = open(file_prefix + 'features.pkl', 'rb')\n",
    "features = pickle.load(pfile)\n",
    "pfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = store_locations(\"../data/atlas_complete.json\")\n",
    "folds = create_folds(min_x, min_y, max_x, max_y)\n",
    "\n",
    "# List of dictionaries containing min_x, max_x, min_y, max_y for each fold\n",
    "fold_boundaries = []\n",
    "for fold in folds:\n",
    "    fold_boundaries.append(get_fold_border(fold))\n",
    "\n",
    "# All edges that belong to the validation fold need to be excluded\n",
    "A_train, b_train = build_feat_label_data(G.unique_edges_file_name, ups, features, fold_boundaries=fold_boundaries, excluded_folds=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying standard scaler \n",
    "# scaler_A = StandardScaler()\n",
    "# scaler_b = StandardScaler()\n",
    "\n",
    "# scaler_A.fit(A_train)\n",
    "# scaled_A = scaler_A.transform(A_train)\n",
    "\n",
    "# transformed_b = np.matrix(b_train).T\n",
    "# scaler_b.fit(transformed_b)\n",
    "# scaled_b = scaler_b.transform(transformed_b)\n",
    "# scaled_b = scaled_b.T[0]\n",
    "\n",
    "# print(scaled_b.shape)\n",
    "# print(scaled_A.shape)\n",
    "# print(b_train.shape)\n",
    "# print(A_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3081696,)\n",
      "(3081696, 7)\n",
      "(3081696,)\n",
      "(3081696, 7)\n"
     ]
    }
   ],
   "source": [
    "# trying minmax scaler\n",
    "scaler_A = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_b = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler_A.fit(A_train)\n",
    "scaled_A = scaler_A.transform(A_train)\n",
    "\n",
    "transformed_b = np.matrix(b_train).T\n",
    "scaler_b.fit(transformed_b)\n",
    "scaled_b = scaler_b.transform(transformed_b)\n",
    "scaled_b = scaled_b.T[0]\n",
    "\n",
    "print(scaled_b.shape)\n",
    "print(scaled_A.shape)\n",
    "print(b_train.shape)\n",
    "print(A_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerA_filename = \"scaler_A.pkl\"\n",
    "scalerb_filename = \"scaler_b.pkl\"\n",
    "pickle.dump(scaler_A, open(scalerA_filename, 'wb'))\n",
    "pickle.dump(scaler_b, open(scalerb_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gboost = GradientBoostingRegressor(random_state=1, n_estimators=25).fit(scaled_A, scaled_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2465356 samples, validate on 616340 samples\n",
      "Epoch 1/256\n",
      " - 2318s - loss: 0.0368 - val_loss: 0.0261\n",
      "Epoch 2/256\n",
      " - 2275s - loss: 0.0369 - val_loss: 0.0258\n",
      "Epoch 3/256\n",
      " - 2225s - loss: 0.0371 - val_loss: 0.0268\n",
      "Epoch 4/256\n",
      " - 2250s - loss: 0.0376 - val_loss: 0.0261\n",
      "Epoch 5/256\n",
      " - 2255s - loss: 0.0377 - val_loss: 0.0254\n",
      "Epoch 6/256\n",
      " - 2235s - loss: 0.0379 - val_loss: 0.0265\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_nn = createNonlinearRegressionNeuralNet(scaled_A, scaled_b, train_proportion = 0.8, first_nodes=64, second_nodes=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032673418346232455"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gboost.train_score_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01008998]\n",
      " [0.00855952]\n",
      " [0.01171956]\n",
      " ...\n",
      " [0.10358842]\n",
      " [0.03706743]\n",
      " [0.01214495]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model_nn.predict(scaled_A)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5233\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in prediction:\n",
    "    if i < 0:\n",
    "        count+=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in scaled_b:\n",
    "    if i < 0:\n",
    "        count+=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = create_ground_truth(input_file, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y, projects_to_remove=projects_to_remove)\n",
    "\n",
    "fold_boundaries = []\n",
    "for fold in folds:\n",
    "    fold_boundaries.append(get_fold_border(fold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 0.25\n",
    "\n",
    "compute_edge_weights_multithread(G, ups, model_gboost, features, 5, file_prefix = file_prefix, scalerX=scalerA_filename, scalerY=scalerb_filename)\n",
    "G.sort_edges()\n",
    "\n",
    "\n",
    "\n",
    "comp_assign = region_segmentation(G, ups, kappa)\n",
    "regions, sizes = extract_regions(comp_assign)\n",
    "num_correct_counter, num_assignments_made, precision, recall = evaluate(locations, regions, ups, ground_truth, threshold=0.3, min_x=fold_boundaries[0][\"min_x\"], max_x=fold_boundaries[0][\"max_x\"], min_y=fold_boundaries[0][\"min_y\"], max_y=fold_boundaries[0][\"max_y\"])\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_edge_weights(G, ups, model_nn, features, scalerX=scalerA_filename, scalerY=scalerb_filename)\n",
    "G.sort_edges()\n",
    "\n",
    "\n",
    "\n",
    "comp_assign = region_segmentation(G, ups, kappa)\n",
    "regions, sizes = extract_regions(comp_assign)\n",
    "num_correct_counter, num_assignments_made, precision, recall = evaluate(locations, regions, ups, ground_truth, threshold=0.3, min_x=fold_boundaries[0][\"min_x\"], max_x=fold_boundaries[0][\"max_x\"], min_y=fold_boundaries[0][\"min_y\"], max_y=fold_boundaries[0][\"max_y\"])\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
