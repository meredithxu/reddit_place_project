{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import operator\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../Python_code\") # go to parent dir\n",
    "from canvas_vis import * \n",
    "from analytics_combined import *\n",
    "from generate_proj_to_remove import *\n",
    "from project_data_analysis import *\n",
    "from user_embedding import *\n",
    "from segmentation import *\n",
    "from evaluation import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the size of the canvas that is being looked at\n",
    "min_x = 0\n",
    "max_x = 1002\n",
    "min_y = 0\n",
    "max_y = 1002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_to_remove = get_list_of_removed_proj(output_filename = \"../data/proj_to_remove.txt\")\n",
    "\n",
    "input_file= \"../data/sorted_tile_placements_proj.csv\"\n",
    "js_filename = \"../data/atlas_complete.json\"\n",
    "\n",
    "names, descriptions = read_picture_names_and_descriptions(js_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = create_ground_truth(input_file, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y, projects_to_remove=projects_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "def different_color(i, j, ups, data=None):\n",
    "    if ups[i][4] == ups[j][4]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "def distance_space(i, j, ups, data=None):\n",
    "    xi = ups[i][2]\n",
    "    yi = ups[i][3]\n",
    "    xj = ups[j][2]\n",
    "    yj = ups[j][3]\n",
    "    \n",
    "    return np.sqrt(pow(xi-xj,2)+pow(yi-yj,2))\n",
    "\n",
    "def distance_time(i, j, ups, data=None):\n",
    "    time_i = ups[i][0]\n",
    "    time_j = ups[j][0]\n",
    "    \n",
    "    return np.sqrt(pow(time_i-time_j,2))\n",
    "\n",
    "def distance_duration(i, j, ups, durations):\n",
    "    return dist_duration(durations[i], durations[j])\n",
    "\n",
    "def distance_color(i, j, ups, conflicts):\n",
    "    color_i = ups[i][4]\n",
    "    color_j = ups[j][4]\n",
    "    \n",
    "    if color_i == color_j:\n",
    "        return 0\n",
    "    else:\n",
    "        max_up = len(ups)\n",
    "        dist = 0\n",
    "        \n",
    "        conf_i = []\n",
    "        if conflicts[i][0] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][0]][4])\n",
    "            \n",
    "        if conflicts[i][1] <= max_up:\n",
    "            conf_i.append(ups[conflicts[i][1]][4])\n",
    "        \n",
    "        conf_j = []\n",
    "        if conflicts[j][0] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][0]][4])\n",
    "            \n",
    "        if conflicts[j][1] <= max_up:\n",
    "            conf_j.append(ups[conflicts[j][1]][4])\n",
    "        \n",
    "        if color_i in conf_j:\n",
    "            dist = dist + 1\n",
    "            \n",
    "        if color_j in conf_i:\n",
    "            dist = dist + 1\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "def distance_user_embedding(i, j, ups, data):\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return np.linalg.norm(data['emb'][user_i_id]-data['emb'][user_j_id])\n",
    "\n",
    "def distance_user_colors(i, j, ups, data):\n",
    "    user_i = ups[i][1]\n",
    "    user_j = ups[j][1]\n",
    "    user_i_id = data['index'][user_i]\n",
    "    user_j_id = data['index'][user_j]\n",
    "    \n",
    "    return (1.-data['emb'][user_i_id].todense() * data['emb'][user_j_id].todense().T)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: ALL file_prefix strings must NOT contain any underscore '\\_' characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_and_updatelist(vertex_connectivity, file_prefix):\n",
    "    '''\n",
    "        Either create the Graph and list of updates with the given *vertex_distance* \n",
    "        or load them from the pickle file if the already exist\n",
    "    '''\n",
    "    G = None\n",
    "    ups = []\n",
    "    if not os.path.exists(file_prefix + 'graph.pkl') or not os.path.exists(file_prefix + 'ups.pkl'):\n",
    "        G, ups = create_graph(input_file, projects_to_remove, vertex_connectivity, min_x, max_x, min_y, max_y, file_prefix=file_prefix)\n",
    "\n",
    "        print(\"num edges = \", G.n_edges)\n",
    "        pfile = open(file_prefix + 'graph.pkl', 'wb')\n",
    "        pickle.dump(G, pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        pfile = open(file_prefix + 'ups.pkl', 'wb')\n",
    "        pickle.dump(ups, pfile)\n",
    "        pfile.close()\n",
    "    else:\n",
    "        pfile = open(file_prefix + 'graph.pkl', 'rb')\n",
    "        G = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "\n",
    "        pfile = open(file_prefix + 'ups.pkl', 'rb')\n",
    "        ups = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "    \n",
    "    return G, ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(G, ups, file_prefix):\n",
    "    features = []\n",
    "    features_filename = file_prefix + 'features.pkl'\n",
    "    if not os.path.exists(features_filename):\n",
    "\n",
    "        conflicts = compute_update_conflicts(ups)\n",
    "        durations = compute_update_durations(ups)\n",
    "        user_color, user_index_color = compute_user_color(ups)\n",
    "\n",
    "        #TODO: We are currently using 40 dimensions, we might need more\n",
    "        # We also need to understand whether these other parameters matter.\n",
    "        ndim=40\n",
    "        threshold=10\n",
    "        total_samples=200\n",
    "        n_negatives=5\n",
    "        n_iterations=10\n",
    "        user_index, emb = embed_users(G, ups, ndim, threshold, total_samples, n_negatives, n_iterations)\n",
    "\n",
    "        features = [{'name': \"different_color\", 'func': different_color, 'data': None}, \n",
    "            {'name': \"distance_space\",  'func': distance_space, 'data': None}, \n",
    "            {'name': \"distance_time\", 'func': distance_time, 'data': None}, \n",
    "            {'name': \"distance_duration\", 'func': distance_duration, 'data': durations}, \n",
    "            {'name': \"distance_color\", 'func': distance_color, 'data': conflicts},\n",
    "            {'name': \"distance_user_embedding\", 'func': distance_user_embedding, 'data': {'index': user_index, 'emb': emb}},\n",
    "            {'name': \"distance_user_colors\", 'func': distance_user_colors, 'data': {'index': user_index_color, 'emb': user_color}}]\n",
    "\n",
    "        pfile = open(features_filename, 'wb')\n",
    "        pickle.dump(features, pfile)\n",
    "        pfile.close()\n",
    "    else:\n",
    "        pfile = open(features_filename, 'rb')\n",
    "        features = pickle.load(pfile)\n",
    "        pfile.close()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vertex_distance(min = 1, max = 4):\n",
    "    for vertex_connectivity in range(min, max+1):\n",
    "        file_prefix = \"dist\" + str(vertex_connectivity)\n",
    "        \n",
    "        G, ups = create_graph_and_updatelist(vertex_connectivity, file_prefix)\n",
    "        features = create_features(G, ups, file_prefix)\n",
    "        \n",
    "        metric_vals = validate_best_model(evaluate, ups, G, features, input_file, projects_to_remove,'recall', ground_truth, min_x, min_y, max_x, max_y, file_prefix=file_prefix, compute_edge_weights = False, load_segmentation = True, load_models = True)\n",
    "        print(metric_vals)\n",
    "        print(\"AVG for vertex distance \" , distance,\":\",(sum(metric_vals)/len(metric_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num edges =  67201881\n",
      "python ../../signet/signet.py -l signet_id.txt -i signet.txt -o signet -d 40 -t 10 -s 200\n",
      "reddit_place_project\n",
      "signet\n",
      "signet.tar.gz\n",
      "\n",
      "avg pos =  0.9041201576605967 , n =  18622078\n",
      "avg neg =  1.363618305312599 , n =  9455122\n",
      "0.23026315789473684\n",
      "0.29850746268656714\n",
      "0.30303030303030304\n",
      "0.27979274611398963\n",
      "0.4539877300613497\n",
      "0.23026315789473684\n",
      "0.29850746268656714\n",
      "0.30303030303030304\n",
      "0.27979274611398963\n",
      "0.4539877300613497\n",
      "0.2608695652173913\n",
      "0.2184873949579832\n",
      "0.30718954248366015\n",
      "0.3104693140794224\n",
      "0.21428571428571427\n",
      "[0.23026315789473684, 0.29850746268656714, 0.30303030303030304, 0.27979274611398963, 0.4539877300613497, 0.23026315789473684, 0.29850746268656714, 0.30303030303030304, 0.27979274611398963, 0.4539877300613497, 0.2608695652173913, 0.2184873949579832, 0.30718954248366015, 0.3104693140794224, 0.21428571428571427]\n",
      "AVG for vertex distance  <module 'scipy.spatial.distance' from '/cs/student/danielshu/.local/lib/python3.6/site-packages/scipy/spatial/distance.py'> : 0.29616428870653766\n",
      "num edges =  169166135\n",
      "python ../../signet/signet.py -l signet_id.txt -i signet.txt -o signet -d 40 -t 10 -s 200\n",
      "reddit_place_project\n",
      "signet\n",
      "signet.tar.gz\n",
      "\n",
      "avg pos =  0.5289558438235539 , n =  40868578\n",
      "avg neg =  0.9594906052145467 , n =  9222257\n"
     ]
    }
   ],
   "source": [
    "evaluate_vertex_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kappas(kappas = [0.2,0.5,0.8,1.0,1.3,1.5,1.9]):\n",
    "    '''\n",
    "        Evaluate the precision and recall given different values of kappa\n",
    "        \n",
    "        This code assumes that evaluate_vertex_distance() has already been called at least once, which\n",
    "        will create and pickle the different models\n",
    "    '''\n",
    "    for kappa in kappas:\n",
    "        # For this test, we will fix vertex connectivity at 1\n",
    "        file_prefix = \"dist1\"\n",
    "        \n",
    "        G, ups = create_graph_and_updatelist(1, file_prefix)\n",
    "        features = create_features(G, ups, file_prefix)\n",
    "        \n",
    "        metric_vals = validate_best_model(evaluate, ups, G, features, input_file, projects_to_remove,'recall', ground_truth, min_x, min_y, max_x, max_y, file_prefix=file_prefix, kappa = kappa, compute_edge_weights = False, load_segmentation = True, load_models = True)\n",
    "        print(metric_vals)\n",
    "        print(\"AVG for kappa values \" , kappa,\":\",(sum(metric_vals)/len(metric_vals)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_kappas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modeltypes():\n",
    "    ''' \n",
    "        Evaluate which modeltype is better\n",
    "            modeltype = 0 will use a sklearn.ensemble.GradientBoostingRegressor\n",
    "            modeltype = 1 will create a keras.models.Sequential neural network\n",
    "            \n",
    "    '''\n",
    "    modeltypes = [0,1]\n",
    "    for modeltype in modeltypes:\n",
    "        # For this test, we will fix vertex connectivity at 1 and fix kappa at 0.3\n",
    "        kappa = 0.3\n",
    "        file_prefix = \"dist1\"\n",
    "        \n",
    "        G, ups = create_graph_and_updatelist(1, file_prefix)\n",
    "        features = create_features(G, ups, file_prefix)\n",
    "        metric_vals = validate_best_model(evaluate, ups, G, features, input_file, projects_to_remove,'recall', ground_truth, min_x, min_y, max_x, max_y, file_prefix=file_prefix, kappa = kappa, compute_edge_weights = True, load_segmentation = True, load_models = True, modeltype = modeltype)\n",
    "        print(metric_vals)\n",
    "        print(\"AVG for kappa values \" , kappa,\":\",(sum(metric_vals)/len(metric_vals)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_modeltypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
